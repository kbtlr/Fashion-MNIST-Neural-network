{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMNMt2C8PcB3UMLWjd5UlQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LflWWBX8pFkQ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')\n","import my_utils as mu\n","import torch\n","import collections\n","from collections import defaultdict\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from IPython import display\n","import numpy as np\n","import pandas as pd\n","import random\n","import math\n","from torch.utils.data.sampler import SubsetRandomSampler\n","d2l = sys.modules[__name__]\n","## Code to train a specific MLP model in recognising Fashion-MNIST datasets.\n","\n","\n","## First, the dataset is retrieved, then transformed to fit our split algorithm.\n","shift = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.1307,), (0.3081,))])\n","batch_size = 256\n","\n","fashion_trainset = datasets.FashionMNIST(root='./fmnist/', train=True,\n","                                         download=True, transform=shift)\n","train_range = len(fashion_trainset)\n","indexes = list(range(train_range))\n","divide = 24000\n","\n","## The data is split randomly in to test and train datasets. The greater our \n","## split is in favour of train, the more accurate our test will be.\n","val = np.random.choice(indexes, size=divide, replace=True)\n","train = list(set(indexes) - set(val))\n","\n","sampler1 = SubsetRandomSampler(train)\n","sampler2 = SubsetRandomSampler(val)\n","\n","## Finally, dataloaders are initialised for later use in the trainer model.\n","train_iter = torch.utils.data.DataLoader(fashion_trainset, batch_size, \n","                                         sampler=sampler1)\n","test_iter = torch.utils.data.DataLoader(fashion_trainset, batch_size,\n","                                         sampler=sampler2)\n","train_features, train_labels = next(iter(train_iter))\n","\n","\n","\n","num_inputs = 49\n","num_hidden = 256\n","num_outputs = 10\n","\n","## Next, we have our stem/backbone/classifier model\n","class mlp(nn.Module):\n","        def __init__(self, num_inputs, num_hidden, num_outputs):\n","            super(mlp, self).__init__()\n","            \n","            ## Initialise our inputs, and the patch variables\n","            self.num_inputs = num_inputs\n","            self.num_hidden = num_hidden\n","            self.num_outputs = num_outputs\n","            self.number = 16\n","            self.dimensions = 7\n","            self.pixels = 49\n","\n","            ## Linear function and 2 sequentially run backbones, with the \n","            ## tranposed inputs added later.\n","            self.side = nn.Linear(self.pixels, num_inputs)\n","\n","            self.backbone1 = nn.Sequential(nn.Linear(num_inputs, num_hidden, True), \n","                                      nn.ReLU(), nn.Linear(num_hidden, num_hidden, True))\n","            self.backbone2 = nn.Sequential(nn.Linear(num_hidden, num_hidden, True), \n","                                      nn.ReLU(), nn.Linear(num_hidden, num_outputs, True))\n","\n","\n","        def forward(self, x):\n","            \n","           ## Stem process to arrange oncoming data from tensors in to small \n","           ## pixels flattened to be side by side.\n","           unfold = nn.Unfold(kernel_size=(7,7), stride=(7,7))\n","           x = x.unfold(2, 7, 7).unfold(3, 7, 7)\n","           tens = []\n","           for patch in x:\n","              tens2 = []\n","              p = patch.flatten()\n","              for patch2 in torch.split(p, self.pixels):\n","                 final_patch = self.side(patch2)\n","                 tens2.append(final_patch)\n","              tens.append(torch.stack(tens2))\n","\n","           ## Data is then individually transposed and put through each backbone\n","           x = torch.stack(tens)\n","           x = torch.transpose(x, 0, 1)\n","           x = self.backbone1(x)\n","           x = torch.transpose(x, 0, 1)\n","           x = self.backbone2(x)\n","           \n","           ## Data is finally run through the classifier\n","           x = torch.mean(x, 1, False)\n","           return x\n","## Here, the weights of the model are initialised.\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        torch.nn.init.normal_(m.weight, std=0.01)\n","        torch.nn.init.zeros_(m.bias)\n","\n","## More variable initialising ahead of running the training model, including the\n","## loss, epochs and Adam optimiser.\n","net = mlp(num_inputs, num_hidden, num_outputs)\n","loss = nn.CrossEntropyLoss()\n","optimiser = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), \n","                       eps=1e-08, weight_decay=0.001)\n","num_epochs = 25\n","\n","## Finally, we use the train model included in the my_utils file. The train_iter \n","## is provided to the model, which then analyses the test_iter data. As more \n","## train_iter data is provided, the model becomes more accurate.\n","mu.train_ch3(net, train_iter, test_iter, loss, num_epochs, optimiser)\n","\n","## Final accuracy is given above our graph animation from the above model.\n","mu.evaluate_accuracy(net, test_iter)\n"]}]}